{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h2><b style=\"color:#30A2FF\">Named Entity Recognition (NER) Model</b></h2>\n",
    "1. Named Entity Recognition (NER)  is one of the widely used application of Natural Language Processing.<br>\n",
    "<b style=\"color:#0E2954\"> 2.NER is a subtask of information extraction that locates and classifies named entities in a text. The named entities could be organizations, persons, locations, times,etc.</b><br>\n",
    "3. NER is Useful in  <b style=\"color:#3330E4\">Serch Engine Efficiency,Recommendation engine etc.</b><br>\n",
    "4. This code is divided into four parts-<br>\n",
    "<em style=\"margin-left:25px\";>a. Data Preprocessing</em><br>\n",
    "<em style=\"margin-left:25px\";>b. Building <b style=\"color:#3330E4\"> Bidirectional LSTM model</b></em><br>\n",
    "<em style=\"margin-left:25px\";>c.Training & Testing of the model using validation.</em><br>\n",
    "<em style=\"margin-left:25px\";>d. Predicting the output of New Sentence </em><br>\n",
    "5.I used open source data-set available on kagel.<br>\n",
    "6.I used open-source library: <b>Tenserflow from Google</b> for model building and designing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pprint as pp\n",
    "import random as rnd\n",
    "from utils1 import *\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b style=\"color:#0E2954\">Part-1. Data Pre-Processing</b></h3>\n",
    "In this part ,I done preprocessing by preforming the following tasks-<br>\n",
    "<em style=\"margin-left:25px\";>a. Loading the data</em><br>\n",
    "<em style=\"margin-left:25px\";>b. Creating Vocabulary and tag-mapping from train data</em><br>\n",
    "<em style=\"margin-left:25px\";>c. Adding Padding to train_data</em><br>\n",
    "<em style=\"margin-left:25px\";>d. Creating Tensors for training and testing dataset</em><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "\n",
      "SENTENCE LABEL: O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_sents = open('data/small/train/sentences.txt', 'r').readline()\n",
    "train_labels = open('data/small/train/labels.txt', 'r').readline()\n",
    "print('SENTENCE:', train_sents)\n",
    "print('SENTENCE LABEL:', train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, tag_map = get_vocab('data/large/words.txt', 'data/large/tags.txt')\n",
    "t_sentences, t_labels, t_size = get_params(\n",
    "    vocab, tag_map, 'data/large/train/sentences.txt', 'data/large/train/labels.txt')\n",
    "v_sentences, v_labels, v_size = get_params(\n",
    "    vocab, tag_map, 'data/large/val/sentences.txt', 'data/large/val/labels.txt')\n",
    "test_sentences, test_labels, test_size = get_params(\n",
    "    vocab, tag_map, 'data/large/test/sentences.txt', 'data/large/test/labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab[\"the\"]: 9\n",
      "padded token: 35180\n"
     ]
    }
   ],
   "source": [
    "print('vocab[\"the\"]:', vocab[\"the\"])\n",
    "# Pad token\n",
    "print('padded token:', vocab['<PAD>'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0E2954\">Some Tags and their sematics:</h3>\n",
    "1. geo : geographical entity<br>\n",
    "2. org : organization<br>\n",
    "3. per : person<br>\n",
    "4. gpe : geopolitical entity<br>\n",
    "5. tim : time indicator<br>\n",
    "6. art : artifact<br>\n",
    "7. eve : event<br>\n",
    "8. nat : natural phenomenon<br>\n",
    "9. O: filler word<br><br>\n",
    "<b style=\"color:#3330E4\">The coding scheme that tags the entities is a minimal one where B- indicates the first token in a multi-token entity, and I- indicates one in the middle of a multi-token entity</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-art': 8,\n",
      " 'B-eve': 14,\n",
      " 'B-geo': 1,\n",
      " 'B-gpe': 2,\n",
      " 'B-nat': 13,\n",
      " 'B-org': 5,\n",
      " 'B-per': 3,\n",
      " 'B-tim': 7,\n",
      " 'I-art': 9,\n",
      " 'I-eve': 15,\n",
      " 'I-geo': 4,\n",
      " 'I-gpe': 11,\n",
      " 'I-nat': 16,\n",
      " 'I-org': 6,\n",
      " 'I-per': 10,\n",
      " 'I-tim': 12,\n",
      " 'O': 0}\n",
      "Number of labels available in tag_map 17\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(tag_map)\n",
    "print(f'Number of labels available in tag_map {len(tag_map)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring information from data\n",
      "\n",
      "The size of training set is 33570\n",
      "The size of validation set is 7194\n",
      "The size of testing of set is 7194\n",
      "Example sentence from training set [7049, 151, 1849, 7, 140, 1902, 21]\n",
      "Example tags line from trainiing set [3, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(f'Exploring information from data\\n')\n",
    "print(f'The size of training set is {t_size}')\n",
    "print(f'The size of validation set is {v_size}')\n",
    "print(f'The size of testing of set is {test_size}')\n",
    "print(f'Example sentence from training set {t_sentences[2000]}')\n",
    "print(f'Example tags line from trainiing set {t_labels[2000]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:#0E2954\">Padding</h4>\n",
    "When training an LSTM using batches, all our input sentences must be the same size. To accomplish this, we set the length of our sentences to a certain number and add the generic <PAD> token to fill all the empty spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxLengthProvider(sentences):\n",
    "    maxi=0\n",
    "    for sentence in sentences:\n",
    "        maxi=max(len(sentence),maxi)\n",
    "    return maxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxLength=max(maxLengthProvider(t_sentences),maxLengthProvider(v_sentences),maxLengthProvider(test_sentences))\n",
    "maxLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sentences = pad_sequences(\n",
    "    t_sentences, padding='post',maxlen=maxLength,value=17.0,truncating='post')\n",
    "t_labels = pad_sequences(\n",
    "    t_labels, padding='post', maxlen=maxLength, truncating='post')\n",
    "v_sentences = pad_sequences(\n",
    "    v_sentences, padding='post',maxlen=maxLength, truncating='post')\n",
    "v_labels = pad_sequences(\n",
    "    v_labels, padding='post', maxlen=maxLength, value=18.0, truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preparing the training dataset.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((t_sentences, t_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Preparing the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((v_sentences, v_labels))\n",
    "val_dataset = val_dataset.batch(64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b style=\"color:#0E2954\">Part-2. Model Building and Desiging</b></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"NER\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 104)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 104, 64)           2251584   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 104, 128)         66048     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " Prediction (Dense)          (None, 104, 17)           2193      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,319,825\n",
      "Trainable params: 2,319,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model buliding\n",
    "input_shape = (maxLength,)\n",
    "inputs = keras.Input(shape=input_shape)\n",
    "Embedding = keras.layers.Embedding(input_dim=len(vocab), output_dim=64,mask_zero=True)(inputs)\n",
    "x1 = keras.layers.Bidirectional(layers.LSTM(64, return_sequences=True))(Embedding)\n",
    "outputs=keras.layers.Dense(units=len(tag_map),name='Prediction')(x1)\n",
    "model=keras.Model(inputs=inputs,outputs=outputs,name='NER')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b style=\"color:#0E2954\">Part-3a.Training on train_dataset</b></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 2.7969\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.1226\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.0735\n",
      "Seen so far: 25664 samples\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.0643\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.0510\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.0475\n",
      "Seen so far: 25664 samples\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 0.0300\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.0301\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.0350\n",
      "Seen so far: 25664 samples\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 0.0330\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.0111\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.0135\n",
      "Seen so far: 25664 samples\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 0.0330\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.0115\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.0137\n",
      "Seen so far: 25664 samples\n"
     ]
    }
   ],
   "source": [
    "#Training Loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            # Logits for this minibatch\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "            \n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b style=\"color:#0E2954\">Part-3b.Testing on val_dataset</b></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(model=model,val_dataset=val_dataset):\n",
    "    return model.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masks(v_sentences=v_sentences):\n",
    "    '''Generating masks to ignore the padding'''\n",
    "    embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)\n",
    "    masked_output = embedding(v_sentences)\n",
    "    return (masked_output._keras_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(test_sentences,test_labels,masks=masks,predictions=predictions):\n",
    "    '''Accuracy of model predictions to ignore the padding'''\n",
    "    predicted_labels = np.argmax(predictions(), axis=2)\n",
    "    x,y=test_labels.shape\n",
    "    return  (np.sum(predicted_labels == test_labels))/(float(np.sum(masks())))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 41s 346ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9482130986673419"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(v_sentences,v_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b style=\"color:#0E2954\">Part-4.Predictions on real world sentences</b></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(givenSentence,model=model,vocab=vocab,maxLength=maxLength,tag_map=tag_map):\n",
    "    s = [vocab[token] if token in vocab else vocab['UNK']\n",
    "         for token in givenSentence.split(' ')]\n",
    "    s = s+[0]*(maxLength-len(s))\n",
    "    batch_data = np.ones((1, len(s)))\n",
    "    batch_data[0] = s\n",
    "    sentence = np.array(batch_data).astype(int)\n",
    "    predictions=model.predict(sentence)\n",
    "    outputs = np.argmax(predictions, axis=2)\n",
    "    pred=[]\n",
    "    labels = list(tag_map.keys())\n",
    "    words=givenSentence.split(' ')\n",
    "    for i,word in enumerate(words):\n",
    "        idx=outputs[0][i]\n",
    "        tag=labels[idx]\n",
    "        if(tag!='O'):\n",
    "            pred.append((word,tag))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "[('Peter', 'B-per'),\n",
      " ('Navarro,', 'I-per'),\n",
      " ('White', 'B-org'),\n",
      " ('House', 'I-org'),\n",
      " ('Sunday', 'B-tim'),\n",
      " ('morning', 'I-tim'),\n",
      " ('White', 'B-org'),\n",
      " ('House', 'I-org')]\n"
     ]
    }
   ],
   "source": [
    "# Trying Example Sentence for testing real world example\n",
    "sentence = \"Peter Navarro, the White House director of trade and manufacturing policy of U.S, said in an interview on Sunday morning that the White House was working to prepare for the possibility of a second wave of the coronavirus in the fall, though he said it wouldnâ€™t necessarily come\"\n",
    "pp.pprint(predict(sentence))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Refernces</b><br>\n",
    "1.Tensorflow documentation-<a href=\"https://www.tensorflow.org/\">Link</a><br>\n",
    "2.Dataset <a href=\"https://www.kaggle.com/datasets/debasisdotcom/name-entity-recognition-ner-dataset\">Link</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
